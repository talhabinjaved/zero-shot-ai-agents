# Experiment Plan Template for Cosine
# This file will be customized by the Cosine agent based on the specific idea
# Cosine will monitor CI steps and auto-iterate until experiments pass
# The agent will refine this template into a concrete experiment plan

stop_on_fail: true

# Global experiment configuration
config:
  random_seed: 42
  log_level: INFO
  output_dir: artifacts
  cache_dir: .cache

# Ordered list of experiment steps
# Cosine will implement code for each step and ensure CI passes
steps:
  - name: environment_setup
    description: "Set up the computational environment and verify dependencies"
    cmd: "python scripts/setup.py --config experiments/config.yaml"
    inputs: []
    resources:
      cpu: 2
      memory_gb: 4
      expected_duration_minutes: 10
    sanity:
      - type: file_exists
        path: artifacts/env_check.json
      - type: json_value
        path: artifacts/env_check.json
        key: python_version
        operator: ">="
        value: "3.8"
    retry: 2
    timeout_minutes: 15

  - name: data_preparation
    description: "Prepare and validate input data for experiments"
    cmd: "python scripts/data_prep.py --config experiments/config.yaml --output artifacts/data/"
    inputs:
      - experiments/config.yaml
    resources:
      cpu: 4
      memory_gb: 8
      expected_duration_minutes: 30
    sanity:
      - type: file_exists
        path: artifacts/data/train.csv
      - type: file_exists
        path: artifacts/data/validation.csv
      - type: json_value
        path: artifacts/data/stats.json
        key: num_samples
        operator: ">"
        value: 100
    retry: 3
    timeout_minutes: 45

  - name: baseline_experiment
    description: "Run baseline experiment for comparison"
    cmd: "python scripts/baseline.py --config experiments/config.yaml --data artifacts/data/ --output artifacts/baseline/"
    inputs:
      - experiments/config.yaml
      - artifacts/data/train.csv
      - artifacts/data/validation.csv
    resources:
      cpu: 4
      memory_gb: 16
      expected_duration_minutes: 60
    sanity:
      - type: file_exists
        path: artifacts/baseline/model.pkl
      - type: file_exists
        path: artifacts/baseline/metrics.json
      - type: json_value
        path: artifacts/baseline/metrics.json
        key: validation_score
        operator: ">"
        value: 0.1  # At least better than random
    retry: 2
    timeout_minutes: 90

  - name: main_experiment
    description: "Run the main experimental implementation"
    cmd: "python scripts/experiment.py --config experiments/config.yaml --data artifacts/data/ --baseline artifacts/baseline/model.pkl --output artifacts/results/"
    inputs:
      - experiments/config.yaml
      - artifacts/data/train.csv
      - artifacts/data/validation.csv
      - artifacts/baseline/model.pkl
    resources:
      cpu: 8
      memory_gb: 32
      expected_duration_minutes: 120
    sanity:
      - type: file_exists
        path: artifacts/results/model.pkl
      - type: file_exists
        path: artifacts/results/metrics.json
      - type: file_exists
        path: artifacts/results/plots/performance.png
      - type: json_value
        path: artifacts/results/metrics.json
        key: test_score
        operator: ">"
        value: 0.5  # Reasonable performance threshold
    retry: 3
    timeout_minutes: 180

  - name: analysis_and_reporting
    description: "Analyze results and generate comprehensive reports"
    cmd: "python scripts/analysis.py --results artifacts/results/ --baseline artifacts/baseline/ --output artifacts/analysis/"
    inputs:
      - artifacts/results/metrics.json
      - artifacts/results/model.pkl
      - artifacts/baseline/metrics.json
    resources:
      cpu: 4
      memory_gb: 16
      expected_duration_minutes: 30
    sanity:
      - type: file_exists
        path: artifacts/analysis/report.md
      - type: file_exists
        path: artifacts/analysis/comparison.json
      - type: file_exists
        path: artifacts/analysis/plots/summary.png
    retry: 1
    timeout_minutes: 45

# Post-processing hooks (optional)
# These run after all steps complete successfully
post_process:
  - name: generate_readme
    description: "Generate comprehensive README from results"
    cmd: "python scripts/generate_readme.py --template README.template.md --results artifacts/ --output README.md"
    inputs:
      - README.template.md
      - artifacts/analysis/report.md
    resources:
      cpu: 2
      memory_gb: 4
      expected_duration_minutes: 5

  - name: cleanup
    description: "Clean up temporary files and optimize storage"
    cmd: "python scripts/cleanup.py --artifacts artifacts/ --keep-essential"
    inputs:
      - artifacts/
    resources:
      cpu: 2
      memory_gb: 4
      expected_duration_minutes: 10
